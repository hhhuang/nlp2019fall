{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training/test data is publicly avaialbe here: http://sighan.cs.uchicago.edu/bakeoff2005/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 708953\n",
      "Number of sentences in the test data: 14432\n"
     ]
    }
   ],
   "source": [
    "raw_train = []\n",
    "raw_test = []\n",
    "with open(\"data/as_training.utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_train.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "with open(\"data/as_testing_gold.utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_test.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "print(\"Number of sentences in the training data: %d\" % len(raw_train))\n",
    "print(\"Number of sentences in the test data: %d\" % len(raw_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/q2/3j47nw291jl5bwz3l9y9cb3r0000gn/T/jieba.cache\n",
      "Loading model cost 0.783 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區長', '青學苑', '多', '開設', '有書法', '、', '插花', '、', '土風', '舞班', '，']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "print(list(jieba.cut(\"\".join(raw_test[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install hanziconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['许多', '社区', '长青', '学苑', '多', '开设', '有', '书法', '、', '插花', '、', '土风舞', '班', '，']\n"
     ]
    }
   ],
   "source": [
    "from hanziconv.hanziconv import HanziConv\n",
    "\n",
    "print(list(jieba.cut(HanziConv.toSimplified(\"\".join(raw_test[0])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞', '班', '，']\n"
     ]
    }
   ],
   "source": [
    "def restore(text, toks):\n",
    "    results = []\n",
    "    offset = 0\n",
    "    for tok in toks:\n",
    "        results.append(text[offset:offset + len(tok)])\n",
    "        offset += len(tok)\n",
    "    return results\n",
    "\n",
    "text = \"\".join(raw_test[0])\n",
    "print(restore(text, list(jieba.cut(HanziConv.toSimplified(text)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Our Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a list of words to a sequence of tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許', '多', '社', '區', '長', '青', '學', '苑', '多', '開', '設', '有', '書', '法', '、', '插', '花', '、', '土', '風', '舞', '班', '，']\n",
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'M', 'M', 'R', 'S']\n"
     ]
    }
   ],
   "source": [
    "def words_to_tags(words):\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tags.append('S')\n",
    "        else:\n",
    "            for i in range(len(word)):\n",
    "                if i == 0:\n",
    "                    tags.append('L')\n",
    "                elif i == len(word) - 1:\n",
    "                    tags.append('R')\n",
    "                else:\n",
    "                    tags.append('M')\n",
    "    return tags\n",
    "    \n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for sent in raw_train:\n",
    "    train_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence as a sequence of characters\n",
    "    train_Y.append(words_to_tags(sent))\n",
    "    \n",
    "for sent in raw_test:\n",
    "    test_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence\n",
    "    test_Y.append(words_to_tags(sent))\n",
    "    \n",
    "print(test_X[0])\n",
    "print(test_Y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CRF model for word segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 708953/708953 [00:54<00:00, 13081.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 20.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 70338\n",
      "Seconds required: 12.666\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 300\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=4.48  loss=9523529.41 active=70226 feature_norm=1.00\n",
      "Iter 2   time=2.19  loss=5960274.10 active=70338 feature_norm=5.08\n",
      "Iter 3   time=4.35  loss=5632639.26 active=70338 feature_norm=5.60\n",
      "Iter 4   time=2.17  loss=5317692.30 active=70338 feature_norm=6.39\n",
      "Iter 5   time=2.16  loss=3919265.17 active=70338 feature_norm=12.31\n",
      "Iter 6   time=2.14  loss=3640476.35 active=70338 feature_norm=14.54\n",
      "Iter 7   time=2.15  loss=3411889.18 active=70338 feature_norm=14.45\n",
      "Iter 8   time=2.15  loss=3244375.69 active=70338 feature_norm=16.20\n",
      "Iter 9   time=2.17  loss=3037585.23 active=70338 feature_norm=18.89\n",
      "Iter 10  time=2.15  loss=2606536.85 active=70338 feature_norm=26.14\n",
      "Iter 11  time=4.29  loss=2535752.26 active=70338 feature_norm=28.63\n",
      "Iter 12  time=2.17  loss=2469514.25 active=70338 feature_norm=29.93\n",
      "Iter 13  time=2.19  loss=2363371.12 active=70338 feature_norm=34.97\n",
      "Iter 14  time=2.18  loss=2296980.14 active=70338 feature_norm=39.03\n",
      "Iter 15  time=4.34  loss=2264629.05 active=70338 feature_norm=41.31\n",
      "Iter 16  time=2.12  loss=2212723.04 active=70338 feature_norm=45.24\n",
      "Iter 17  time=2.14  loss=2163880.41 active=70338 feature_norm=48.40\n",
      "Iter 18  time=2.15  loss=2069716.14 active=70338 feature_norm=53.55\n",
      "Iter 19  time=4.33  loss=2033233.69 active=70338 feature_norm=54.42\n",
      "Iter 20  time=2.18  loss=1985223.24 active=70338 feature_norm=54.52\n",
      "Iter 21  time=2.18  loss=1948289.00 active=70338 feature_norm=53.29\n",
      "Iter 22  time=2.14  loss=1921669.59 active=70338 feature_norm=51.41\n",
      "Iter 23  time=2.16  loss=1885144.81 active=70338 feature_norm=49.16\n",
      "Iter 24  time=2.13  loss=1845681.01 active=70338 feature_norm=48.65\n",
      "Iter 25  time=2.15  loss=1824076.42 active=70338 feature_norm=49.34\n",
      "Iter 26  time=2.14  loss=1791517.92 active=70338 feature_norm=50.72\n",
      "Iter 27  time=2.17  loss=1770578.43 active=70338 feature_norm=52.18\n",
      "Iter 28  time=2.13  loss=1735479.07 active=70338 feature_norm=55.50\n",
      "Iter 29  time=4.37  loss=1722607.16 active=70338 feature_norm=56.17\n",
      "Iter 30  time=2.19  loss=1698328.79 active=70338 feature_norm=58.39\n",
      "Iter 31  time=2.20  loss=1674686.89 active=70338 feature_norm=60.43\n",
      "Iter 32  time=2.15  loss=1656614.45 active=70338 feature_norm=61.54\n",
      "Iter 33  time=2.17  loss=1641392.50 active=70338 feature_norm=61.96\n",
      "Iter 34  time=2.16  loss=1603849.03 active=70338 feature_norm=64.11\n",
      "Iter 35  time=4.33  loss=1591484.03 active=70338 feature_norm=65.05\n",
      "Iter 36  time=2.17  loss=1577521.47 active=70338 feature_norm=66.91\n",
      "Iter 37  time=2.18  loss=1568734.04 active=70338 feature_norm=68.36\n",
      "Iter 38  time=2.22  loss=1561491.45 active=70338 feature_norm=69.39\n",
      "Iter 39  time=2.13  loss=1555208.94 active=70338 feature_norm=69.82\n",
      "Iter 40  time=2.16  loss=1537183.52 active=70338 feature_norm=72.42\n",
      "Iter 41  time=4.29  loss=1532008.35 active=70338 feature_norm=72.72\n",
      "Iter 42  time=2.15  loss=1525066.64 active=70338 feature_norm=72.91\n",
      "Iter 43  time=2.27  loss=1515257.98 active=70338 feature_norm=72.77\n",
      "Iter 44  time=2.25  loss=1508153.99 active=70338 feature_norm=72.94\n",
      "Iter 45  time=2.17  loss=1496545.21 active=70338 feature_norm=73.63\n",
      "Iter 46  time=2.13  loss=1488766.00 active=70338 feature_norm=75.49\n",
      "Iter 47  time=2.13  loss=1477819.00 active=70338 feature_norm=76.07\n",
      "Iter 48  time=2.12  loss=1473284.68 active=70338 feature_norm=76.57\n",
      "Iter 49  time=2.14  loss=1465513.15 active=70338 feature_norm=77.90\n",
      "Iter 50  time=2.14  loss=1452087.22 active=70338 feature_norm=80.26\n",
      "Iter 51  time=2.16  loss=1444126.60 active=70338 feature_norm=82.52\n",
      "Iter 52  time=2.20  loss=1434795.63 active=70338 feature_norm=83.52\n",
      "Iter 53  time=2.15  loss=1427562.38 active=70338 feature_norm=83.94\n",
      "Iter 54  time=2.17  loss=1422518.30 active=70338 feature_norm=84.35\n",
      "Iter 55  time=4.28  loss=1420122.10 active=70338 feature_norm=84.71\n",
      "Iter 56  time=2.12  loss=1418342.33 active=70338 feature_norm=84.83\n",
      "Iter 57  time=2.13  loss=1414799.89 active=70338 feature_norm=85.24\n",
      "Iter 58  time=2.15  loss=1411428.13 active=70338 feature_norm=85.92\n",
      "Iter 59  time=2.16  loss=1405049.58 active=70338 feature_norm=87.37\n",
      "Iter 60  time=4.26  loss=1402232.62 active=70338 feature_norm=88.22\n",
      "Iter 61  time=2.13  loss=1398261.14 active=70338 feature_norm=88.89\n",
      "Iter 62  time=2.16  loss=1395012.75 active=70338 feature_norm=89.34\n",
      "Iter 63  time=2.16  loss=1392574.35 active=70338 feature_norm=89.57\n",
      "Iter 64  time=2.11  loss=1389391.90 active=70338 feature_norm=89.80\n",
      "Iter 65  time=2.13  loss=1384019.20 active=70338 feature_norm=90.65\n",
      "Iter 66  time=2.10  loss=1380581.77 active=70338 feature_norm=91.09\n",
      "Iter 67  time=2.11  loss=1375858.31 active=70338 feature_norm=91.82\n",
      "Iter 68  time=2.11  loss=1372240.81 active=70338 feature_norm=92.64\n",
      "Iter 69  time=2.12  loss=1368398.36 active=70338 feature_norm=92.81\n",
      "Iter 70  time=2.10  loss=1365225.15 active=70338 feature_norm=92.93\n",
      "Iter 71  time=2.15  loss=1364082.11 active=70338 feature_norm=93.25\n",
      "Iter 72  time=2.12  loss=1362158.62 active=70338 feature_norm=93.28\n",
      "Iter 73  time=2.13  loss=1359716.88 active=70338 feature_norm=93.52\n",
      "Iter 74  time=2.12  loss=1357900.60 active=70338 feature_norm=93.81\n",
      "Iter 75  time=2.13  loss=1354831.20 active=70338 feature_norm=94.19\n",
      "Iter 76  time=2.14  loss=1351322.40 active=70338 feature_norm=94.97\n",
      "Iter 77  time=2.11  loss=1348560.11 active=70338 feature_norm=95.19\n",
      "Iter 78  time=2.12  loss=1346392.41 active=70338 feature_norm=95.21\n",
      "Iter 79  time=2.12  loss=1343857.36 active=70338 feature_norm=95.29\n",
      "Iter 80  time=2.13  loss=1342044.59 active=70338 feature_norm=95.74\n",
      "Iter 81  time=2.11  loss=1340415.55 active=70338 feature_norm=95.71\n",
      "Iter 82  time=2.10  loss=1338395.51 active=70338 feature_norm=95.85\n",
      "Iter 83  time=2.18  loss=1336833.25 active=70338 feature_norm=96.11\n",
      "Iter 84  time=2.14  loss=1334951.19 active=70338 feature_norm=96.57\n",
      "Iter 85  time=2.16  loss=1332940.81 active=70338 feature_norm=96.77\n",
      "Iter 86  time=2.11  loss=1331778.93 active=70338 feature_norm=96.63\n",
      "Iter 87  time=2.17  loss=1329625.54 active=70338 feature_norm=96.36\n",
      "Iter 88  time=4.26  loss=1328893.42 active=70338 feature_norm=96.40\n",
      "Iter 89  time=2.15  loss=1327589.47 active=70338 feature_norm=96.37\n",
      "Iter 90  time=2.26  loss=1326607.72 active=70338 feature_norm=96.49\n",
      "Iter 91  time=2.17  loss=1326070.94 active=70338 feature_norm=96.63\n",
      "Iter 92  time=2.12  loss=1324863.72 active=70338 feature_norm=96.96\n",
      "Iter 93  time=2.12  loss=1322601.55 active=70338 feature_norm=97.49\n",
      "Iter 94  time=4.36  loss=1321422.58 active=70338 feature_norm=97.84\n",
      "Iter 95  time=2.13  loss=1318754.99 active=70338 feature_norm=98.38\n",
      "Iter 96  time=2.13  loss=1316716.23 active=70338 feature_norm=98.63\n",
      "Iter 97  time=4.30  loss=1315943.13 active=70338 feature_norm=98.81\n",
      "Iter 98  time=2.16  loss=1314475.99 active=70338 feature_norm=98.88\n",
      "Iter 99  time=2.17  loss=1312415.57 active=70338 feature_norm=99.60\n",
      "Iter 100 time=2.14  loss=1311258.23 active=70338 feature_norm=99.60\n",
      "Iter 101 time=2.17  loss=1310172.19 active=70338 feature_norm=99.71\n",
      "Iter 102 time=2.17  loss=1309293.84 active=70338 feature_norm=99.74\n",
      "Iter 103 time=2.15  loss=1307554.06 active=70338 feature_norm=100.14\n",
      "Iter 104 time=2.15  loss=1306140.99 active=70338 feature_norm=100.89\n",
      "Iter 105 time=2.13  loss=1305951.64 active=70338 feature_norm=101.51\n",
      "Iter 106 time=2.12  loss=1304922.57 active=70338 feature_norm=101.51\n",
      "Iter 107 time=2.15  loss=1304673.58 active=70338 feature_norm=101.55\n",
      "Iter 108 time=2.15  loss=1303660.84 active=70338 feature_norm=101.86\n",
      "Iter 109 time=2.15  loss=1301692.05 active=70338 feature_norm=102.41\n",
      "Iter 110 time=4.28  loss=1300690.58 active=70338 feature_norm=103.07\n",
      "Iter 111 time=2.21  loss=1299187.65 active=70338 feature_norm=103.31\n",
      "Iter 112 time=2.19  loss=1298142.92 active=70338 feature_norm=103.46\n",
      "Iter 113 time=4.34  loss=1297749.46 active=70338 feature_norm=103.57\n",
      "Iter 114 time=2.28  loss=1297187.80 active=70338 feature_norm=103.62\n",
      "Iter 115 time=2.20  loss=1296032.46 active=70338 feature_norm=103.76\n",
      "Iter 116 time=2.27  loss=1294442.34 active=70338 feature_norm=104.02\n",
      "Iter 117 time=2.21  loss=1292938.80 active=70338 feature_norm=104.63\n",
      "Iter 118 time=2.24  loss=1291491.69 active=70338 feature_norm=105.08\n",
      "Iter 119 time=2.22  loss=1290652.53 active=70338 feature_norm=105.21\n",
      "Iter 120 time=2.19  loss=1289658.43 active=70338 feature_norm=105.50\n",
      "Iter 121 time=2.21  loss=1288898.07 active=70338 feature_norm=105.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 122 time=2.15  loss=1287876.14 active=70338 feature_norm=105.85\n",
      "Iter 123 time=2.18  loss=1285259.39 active=70338 feature_norm=106.74\n",
      "Iter 124 time=2.18  loss=1283987.74 active=70338 feature_norm=107.11\n",
      "Iter 125 time=2.17  loss=1283152.19 active=70338 feature_norm=107.31\n",
      "Iter 126 time=4.34  loss=1282309.57 active=70338 feature_norm=107.70\n",
      "Iter 127 time=2.22  loss=1281158.09 active=70338 feature_norm=108.08\n",
      "Iter 128 time=2.29  loss=1280696.61 active=70338 feature_norm=108.27\n",
      "Iter 129 time=2.34  loss=1280210.39 active=70338 feature_norm=108.91\n",
      "Iter 130 time=2.23  loss=1279518.39 active=70338 feature_norm=108.85\n",
      "Iter 131 time=2.13  loss=1279258.60 active=70338 feature_norm=108.85\n",
      "Iter 132 time=2.27  loss=1278730.65 active=70338 feature_norm=109.00\n",
      "Iter 133 time=2.20  loss=1278041.12 active=70338 feature_norm=109.40\n",
      "Iter 134 time=2.18  loss=1277931.12 active=70338 feature_norm=109.74\n",
      "Iter 135 time=2.14  loss=1276438.89 active=70338 feature_norm=110.42\n",
      "Iter 136 time=2.15  loss=1275877.76 active=70338 feature_norm=110.54\n",
      "Iter 137 time=2.15  loss=1274958.32 active=70338 feature_norm=110.99\n",
      "Iter 138 time=4.37  loss=1274580.45 active=70338 feature_norm=111.11\n",
      "Iter 139 time=2.19  loss=1274178.00 active=70338 feature_norm=111.21\n",
      "Iter 140 time=2.14  loss=1273473.10 active=70338 feature_norm=111.42\n",
      "Iter 141 time=2.19  loss=1272953.09 active=70338 feature_norm=111.67\n",
      "Iter 142 time=2.17  loss=1272268.61 active=70338 feature_norm=112.16\n",
      "Iter 143 time=2.16  loss=1271560.59 active=70338 feature_norm=112.75\n",
      "Iter 144 time=2.19  loss=1270989.69 active=70338 feature_norm=112.82\n",
      "Iter 145 time=2.14  loss=1269975.07 active=70338 feature_norm=113.17\n",
      "Iter 146 time=2.26  loss=1269478.48 active=70338 feature_norm=113.55\n",
      "Iter 147 time=2.21  loss=1268805.48 active=70338 feature_norm=113.99\n",
      "Iter 148 time=2.21  loss=1268075.11 active=70338 feature_norm=114.54\n",
      "Iter 149 time=2.20  loss=1267487.12 active=70338 feature_norm=114.97\n",
      "Iter 150 time=2.19  loss=1267209.43 active=70338 feature_norm=115.80\n",
      "Iter 151 time=2.17  loss=1266432.90 active=70338 feature_norm=115.68\n",
      "Iter 152 time=2.31  loss=1266048.91 active=70338 feature_norm=115.70\n",
      "Iter 153 time=2.20  loss=1265537.96 active=70338 feature_norm=115.91\n",
      "Iter 154 time=4.49  loss=1265218.36 active=70338 feature_norm=116.22\n",
      "Iter 155 time=2.19  loss=1264681.75 active=70338 feature_norm=116.56\n",
      "Iter 156 time=2.11  loss=1264126.03 active=70338 feature_norm=116.99\n",
      "Iter 157 time=2.11  loss=1263583.91 active=70338 feature_norm=117.49\n",
      "Iter 158 time=2.10  loss=1263104.28 active=70338 feature_norm=117.89\n",
      "Iter 159 time=2.11  loss=1262742.45 active=70338 feature_norm=118.02\n",
      "Iter 160 time=2.14  loss=1262067.59 active=70338 feature_norm=118.37\n",
      "Iter 161 time=2.13  loss=1261530.34 active=70338 feature_norm=118.82\n",
      "Iter 162 time=2.11  loss=1261175.76 active=70338 feature_norm=119.12\n",
      "Iter 163 time=2.20  loss=1260743.23 active=70338 feature_norm=119.13\n",
      "Iter 164 time=2.22  loss=1260201.96 active=70338 feature_norm=119.39\n",
      "Iter 165 time=2.22  loss=1259772.99 active=70338 feature_norm=119.74\n",
      "Iter 166 time=2.28  loss=1259366.83 active=70338 feature_norm=120.56\n",
      "Iter 167 time=2.34  loss=1258634.53 active=70338 feature_norm=120.96\n",
      "Iter 168 time=2.15  loss=1258325.80 active=70338 feature_norm=120.97\n",
      "Iter 169 time=2.16  loss=1257641.79 active=70338 feature_norm=121.32\n",
      "Iter 170 time=4.45  loss=1257470.88 active=70338 feature_norm=121.47\n",
      "Iter 171 time=2.19  loss=1257267.85 active=70338 feature_norm=121.60\n",
      "Iter 172 time=2.26  loss=1257016.58 active=70338 feature_norm=121.75\n",
      "Iter 173 time=2.13  loss=1256353.38 active=70338 feature_norm=122.25\n",
      "Iter 174 time=2.13  loss=1255822.22 active=70338 feature_norm=123.04\n",
      "Iter 175 time=2.11  loss=1255256.96 active=70338 feature_norm=123.46\n",
      "Iter 176 time=2.11  loss=1254985.39 active=70338 feature_norm=123.36\n",
      "Iter 177 time=2.13  loss=1254656.82 active=70338 feature_norm=123.36\n",
      "Iter 178 time=2.11  loss=1254341.75 active=70338 feature_norm=123.51\n",
      "Iter 179 time=2.11  loss=1253968.28 active=70338 feature_norm=123.98\n",
      "Iter 180 time=2.11  loss=1253543.18 active=70338 feature_norm=124.33\n",
      "Iter 181 time=2.12  loss=1253375.64 active=70338 feature_norm=124.36\n",
      "Iter 182 time=2.12  loss=1253090.15 active=70338 feature_norm=124.56\n",
      "Iter 183 time=2.12  loss=1252751.05 active=70338 feature_norm=124.83\n",
      "Iter 184 time=2.13  loss=1252359.18 active=70338 feature_norm=125.30\n",
      "Iter 185 time=2.13  loss=1252057.97 active=70338 feature_norm=125.64\n",
      "Iter 186 time=2.15  loss=1251743.52 active=70338 feature_norm=125.68\n",
      "Iter 187 time=2.13  loss=1251330.33 active=70338 feature_norm=125.71\n",
      "Iter 188 time=2.26  loss=1251002.49 active=70338 feature_norm=125.89\n",
      "Iter 189 time=2.12  loss=1250588.82 active=70338 feature_norm=126.18\n",
      "Iter 190 time=2.12  loss=1250292.30 active=70338 feature_norm=126.34\n",
      "Iter 191 time=2.28  loss=1249698.47 active=70338 feature_norm=126.77\n",
      "Iter 192 time=2.33  loss=1249258.31 active=70338 feature_norm=127.18\n",
      "Iter 193 time=2.16  loss=1248926.63 active=70338 feature_norm=127.30\n",
      "Iter 194 time=2.13  loss=1248635.30 active=70338 feature_norm=127.40\n",
      "Iter 195 time=4.26  loss=1248516.84 active=70338 feature_norm=127.55\n",
      "Iter 196 time=2.12  loss=1248344.86 active=70338 feature_norm=127.64\n",
      "Iter 197 time=2.12  loss=1247991.58 active=70338 feature_norm=127.93\n",
      "Iter 198 time=2.11  loss=1247771.98 active=70338 feature_norm=128.10\n",
      "Iter 199 time=2.12  loss=1247376.00 active=70338 feature_norm=128.36\n",
      "Iter 200 time=2.19  loss=1246873.57 active=70338 feature_norm=128.60\n",
      "Iter 201 time=2.20  loss=1246658.85 active=70338 feature_norm=128.76\n",
      "Iter 202 time=2.12  loss=1246403.21 active=70338 feature_norm=128.62\n",
      "Iter 203 time=2.12  loss=1246222.44 active=70338 feature_norm=128.48\n",
      "Iter 204 time=2.12  loss=1245927.97 active=70338 feature_norm=128.40\n",
      "Iter 205 time=2.13  loss=1245625.51 active=70338 feature_norm=128.30\n",
      "Iter 206 time=2.13  loss=1245473.45 active=70338 feature_norm=128.39\n",
      "Iter 207 time=2.12  loss=1245288.29 active=70338 feature_norm=128.51\n",
      "Iter 208 time=2.11  loss=1245036.69 active=70338 feature_norm=128.63\n",
      "Iter 209 time=2.11  loss=1244768.39 active=70338 feature_norm=128.85\n",
      "Iter 210 time=2.13  loss=1244438.43 active=70338 feature_norm=128.89\n",
      "Iter 211 time=2.12  loss=1244235.11 active=70338 feature_norm=128.83\n",
      "Iter 212 time=2.10  loss=1243845.13 active=70338 feature_norm=128.76\n",
      "Iter 213 time=2.12  loss=1243405.36 active=70338 feature_norm=128.82\n",
      "Iter 214 time=4.24  loss=1243360.97 active=70338 feature_norm=128.83\n",
      "Iter 215 time=2.12  loss=1243163.63 active=70338 feature_norm=128.93\n",
      "Iter 216 time=2.11  loss=1242947.93 active=70338 feature_norm=129.01\n",
      "Iter 217 time=2.16  loss=1242599.23 active=70338 feature_norm=129.35\n",
      "Iter 218 time=2.16  loss=1242493.88 active=70338 feature_norm=129.45\n",
      "Iter 219 time=2.13  loss=1242329.26 active=70338 feature_norm=129.37\n",
      "Iter 220 time=2.12  loss=1242260.65 active=70338 feature_norm=129.36\n",
      "Iter 221 time=2.12  loss=1242038.38 active=70338 feature_norm=129.41\n",
      "Iter 222 time=2.17  loss=1241853.71 active=70338 feature_norm=129.54\n",
      "Iter 223 time=4.29  loss=1241753.89 active=70338 feature_norm=129.62\n",
      "Iter 224 time=2.14  loss=1241578.56 active=70338 feature_norm=129.79\n",
      "Iter 225 time=2.23  loss=1241417.07 active=70338 feature_norm=129.92\n",
      "Iter 226 time=2.19  loss=1241178.58 active=70338 feature_norm=130.11\n",
      "Iter 227 time=2.19  loss=1240895.07 active=70338 feature_norm=130.36\n",
      "Iter 228 time=2.15  loss=1240663.68 active=70338 feature_norm=130.50\n",
      "Iter 229 time=2.13  loss=1240469.51 active=70338 feature_norm=130.49\n",
      "Iter 230 time=2.13  loss=1240315.83 active=70338 feature_norm=130.45\n",
      "Iter 231 time=4.25  loss=1240259.80 active=70338 feature_norm=130.44\n",
      "Iter 232 time=2.13  loss=1240163.18 active=70338 feature_norm=130.43\n",
      "Iter 233 time=2.12  loss=1239992.05 active=70338 feature_norm=130.49\n",
      "Iter 234 time=2.14  loss=1239786.37 active=70338 feature_norm=130.59\n",
      "Iter 235 time=2.13  loss=1239542.45 active=70338 feature_norm=130.74\n",
      "Iter 236 time=2.12  loss=1239442.96 active=70338 feature_norm=131.03\n",
      "Iter 237 time=2.13  loss=1238965.11 active=70338 feature_norm=131.09\n",
      "Iter 238 time=2.13  loss=1238777.02 active=70338 feature_norm=131.12\n",
      "Iter 239 time=2.15  loss=1238629.47 active=70338 feature_norm=131.18\n",
      "Iter 240 time=4.33  loss=1238556.19 active=70338 feature_norm=131.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 241 time=2.13  loss=1238402.99 active=70338 feature_norm=131.31\n",
      "Iter 242 time=2.15  loss=1238034.21 active=70338 feature_norm=131.50\n",
      "Iter 243 time=2.17  loss=1237771.79 active=70338 feature_norm=131.70\n",
      "Iter 244 time=2.13  loss=1237598.45 active=70338 feature_norm=131.86\n",
      "Iter 245 time=2.15  loss=1237430.00 active=70338 feature_norm=131.92\n",
      "Iter 246 time=2.15  loss=1237184.25 active=70338 feature_norm=132.01\n",
      "Iter 247 time=2.16  loss=1237140.86 active=70338 feature_norm=132.24\n",
      "Iter 248 time=2.14  loss=1236922.59 active=70338 feature_norm=132.22\n",
      "Iter 249 time=2.14  loss=1236846.51 active=70338 feature_norm=132.25\n",
      "Iter 250 time=2.13  loss=1236786.50 active=70338 feature_norm=132.32\n",
      "Iter 251 time=2.14  loss=1236682.09 active=70338 feature_norm=132.38\n",
      "Iter 252 time=2.12  loss=1236417.66 active=70338 feature_norm=132.61\n",
      "Iter 253 time=4.27  loss=1236304.73 active=70338 feature_norm=132.63\n",
      "Iter 254 time=2.12  loss=1236138.68 active=70338 feature_norm=132.70\n",
      "Iter 255 time=2.13  loss=1235968.68 active=70338 feature_norm=132.76\n",
      "Iter 256 time=2.12  loss=1235875.55 active=70338 feature_norm=132.73\n",
      "Iter 257 time=2.12  loss=1235769.46 active=70338 feature_norm=132.71\n",
      "Iter 258 time=2.12  loss=1235585.46 active=70338 feature_norm=132.79\n",
      "Iter 259 time=2.11  loss=1235448.11 active=70338 feature_norm=132.84\n",
      "Iter 260 time=2.12  loss=1235288.70 active=70338 feature_norm=132.97\n",
      "Iter 261 time=2.12  loss=1235038.89 active=70338 feature_norm=133.15\n",
      "Iter 262 time=2.12  loss=1234875.06 active=70338 feature_norm=133.31\n",
      "Iter 263 time=2.24  loss=1234682.87 active=70338 feature_norm=133.40\n",
      "Iter 264 time=2.19  loss=1234558.24 active=70338 feature_norm=133.38\n",
      "Iter 265 time=2.13  loss=1234402.79 active=70338 feature_norm=133.36\n",
      "Iter 266 time=2.12  loss=1234212.62 active=70338 feature_norm=133.41\n",
      "Iter 267 time=2.13  loss=1233890.12 active=70338 feature_norm=133.53\n",
      "Iter 268 time=2.14  loss=1233747.46 active=70338 feature_norm=133.69\n",
      "Iter 269 time=2.14  loss=1233605.53 active=70338 feature_norm=133.67\n",
      "Iter 270 time=2.18  loss=1233542.02 active=70338 feature_norm=133.67\n",
      "Iter 271 time=2.14  loss=1233435.67 active=70338 feature_norm=133.69\n",
      "Iter 272 time=2.16  loss=1233301.06 active=70338 feature_norm=133.72\n",
      "Iter 273 time=2.14  loss=1233213.02 active=70338 feature_norm=133.76\n",
      "Iter 274 time=2.11  loss=1233127.66 active=70338 feature_norm=133.81\n",
      "Iter 275 time=2.12  loss=1233015.78 active=70338 feature_norm=133.84\n",
      "Iter 276 time=2.11  loss=1232864.29 active=70338 feature_norm=133.89\n",
      "Iter 277 time=2.14  loss=1232716.78 active=70338 feature_norm=133.91\n",
      "Iter 278 time=2.14  loss=1232630.74 active=70338 feature_norm=133.96\n",
      "Iter 279 time=2.15  loss=1232521.16 active=70338 feature_norm=133.92\n",
      "Iter 280 time=2.15  loss=1232489.19 active=70338 feature_norm=133.91\n",
      "Iter 281 time=2.13  loss=1232375.60 active=70338 feature_norm=133.91\n",
      "Iter 282 time=2.15  loss=1232223.27 active=70338 feature_norm=133.88\n",
      "Iter 283 time=2.15  loss=1232086.55 active=70338 feature_norm=133.93\n",
      "Iter 284 time=2.16  loss=1231981.60 active=70338 feature_norm=133.98\n",
      "Iter 285 time=2.14  loss=1231877.01 active=70338 feature_norm=134.03\n",
      "Iter 286 time=2.11  loss=1231801.21 active=70338 feature_norm=134.04\n",
      "Iter 287 time=4.21  loss=1231704.92 active=70338 feature_norm=134.05\n",
      "Iter 288 time=2.13  loss=1231578.63 active=70338 feature_norm=134.05\n",
      "Iter 289 time=2.12  loss=1231522.63 active=70338 feature_norm=134.06\n",
      "Iter 290 time=2.12  loss=1231443.75 active=70338 feature_norm=134.08\n",
      "Iter 291 time=4.32  loss=1231397.13 active=70338 feature_norm=134.10\n",
      "Iter 292 time=2.12  loss=1231316.25 active=70338 feature_norm=134.12\n",
      "Iter 293 time=2.19  loss=1231208.11 active=70338 feature_norm=134.15\n",
      "Iter 294 time=2.19  loss=1231138.60 active=70338 feature_norm=134.18\n",
      "Iter 295 time=2.18  loss=1231111.83 active=70338 feature_norm=134.20\n",
      "Iter 296 time=2.20  loss=1231060.22 active=70338 feature_norm=134.19\n",
      "Iter 297 time=2.15  loss=1231028.63 active=70338 feature_norm=134.18\n",
      "Iter 298 time=2.14  loss=1230970.80 active=70338 feature_norm=134.18\n",
      "Iter 299 time=2.14  loss=1230867.54 active=70338 feature_norm=134.19\n",
      "Iter 300 time=4.43  loss=1230812.33 active=70338 feature_norm=134.21\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 707.528\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 70338 (70338)\n",
      "Number of active attributes: 23948 (42274)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.106\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=None, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=300,\n",
       "  max_linesearch=None, min_freq=20, model_filename=None, num_memories=None,\n",
       "  pa_type=None, period=None, trainer_cls=None, variance=None, verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "\n",
    "def extract_sent_features(x):\n",
    "    sent_features = []\n",
    "    for i in range(len(x)):\n",
    "        sent_features.append(extract_char_features(x, i))\n",
    "    return sent_features\n",
    "    \n",
    "def extract_char_features(sent, position):\n",
    "    char_features = {}\n",
    "    for i in range(-3, 4):\n",
    "        if len(sent) > position + i >= 0:\n",
    "            char_features['char_at_%d' % i] = sent[position + i]\n",
    "    return char_features\n",
    "\n",
    "crf_tagger = sklearn_crfsuite.CRF(algorithm='lbfgs', min_freq=20, max_iterations=300, verbose=True)\n",
    "\n",
    "feature_X = []\n",
    "for x in train_X:\n",
    "    feature_X.append(extract_sent_features(x))\n",
    "crf_tagger.fit(feature_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['法國', '總統', '馬克宏', '已', '到', '現場', '勘災', '，', '初步', '傳出', '火警', '可能', '與', '目前', '聖母院', '的', '維修', '工程', '有關', '。']\n"
     ]
    }
   ],
   "source": [
    "def segment(sent):\n",
    "    tags = crf_tagger.predict_single(extract_sent_features(list(sent)))\n",
    "    tokens = []\n",
    "    tok = \"\"\n",
    "    for ch, tag in zip(list(sent), tags):\n",
    "        if tag in ['S', 'L'] and tok != \"\":\n",
    "            tokens.append(tok)\n",
    "            tok = \"\"\n",
    "        tok += ch\n",
    "    if tok:\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "            \n",
    "print(segment(\"法國總統馬克宏已到現場勘災，初步傳出火警可能與目前聖母院的維修工程有關。\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer for CWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(actual_toks, pred_toks):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    p = 0\n",
    "    q = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    while i < len(actual_toks) and j < len(pred_toks):\n",
    "        if p == q:\n",
    "            if actual_toks[i] == pred_toks[j]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            p += len(actual_toks[i])\n",
    "            q += len(pred_toks[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif p < q:\n",
    "            p += len(actual_toks[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            q += len(pred_toks[j])\n",
    "            j += 1\n",
    "    return tp, fp, len(actual_toks)\n",
    "    \n",
    "def score(actual_sents, pred_sents):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total = 0\n",
    "    for actual_toks, pred_toks in zip(actual_sents, pred_sents):\n",
    "        tp_, fp_, total_ = compare(actual_toks, pred_toks)\n",
    "        tp += tp_\n",
    "        fp += fp_\n",
    "        total += total_\n",
    "    recall = float(tp) / total\n",
    "    precision = float(tp) / (tp + fp)\n",
    "    f1 = 2.0 * recall * precision / (recall + precision)\n",
    "    return recall, precision, f1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "['許多', '社區長', '青學', '苑多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "(0.8590559143994259, 0.8518791094145525, 0.8554524597486447)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "for sent in raw_test:\n",
    "    pred.append(segment(\"\".join(sent)))\n",
    "    actual.append(sent)\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "\n",
    "print(score(actual, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared with jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞', '班', '，']\n",
      "(0.8148284073856593, 0.8291644535918204, 0.8219339234591463)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "fout = open(\"jieba.out\", \"w\")\n",
    "for sent in raw_test:\n",
    "    text = \"\".join(sent)\n",
    "    r = list(jieba.cut(HanziConv.toSimplified(text)))\n",
    "    r = restore(text, r)\n",
    "    fout.write(\" \".join(r) + \"\\n\")\n",
    "    pred.append(r)\n",
    "    actual.append(sent)\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "\n",
    "print(score(actual, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"zh_gsd\" for language \"zh\".\n",
      "Would you like to download the models for: zh_gsd now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: /Users/hhhuang/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: zh_gsd\n",
      "Download location: /Users/hhhuang/stanfordnlp_resources/zh_gsd_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234M/234M [06:54<00:00, 568kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/hhhuang/stanfordnlp_resources/zh_gsd_models.zip\n",
      "Extracting models file for: zh_gsd\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "stanfordnlp.download('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/Users/hhhuang/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Token index=1;words=[<Word index=1;text=许多;lemma=许多;upos=PROPN;xpos=NNP;feats=_;governor=4;dependency_relation=nmod>]>,\n",
       " <Token index=2;words=[<Word index=2;text=社区;lemma=社区;upos=NOUN;xpos=NN;feats=_;governor=4;dependency_relation=nmod>]>,\n",
       " <Token index=3;words=[<Word index=3;text=长青;lemma=长青;upos=ADJ;xpos=JJ;feats=_;governor=4;dependency_relation=amod>]>,\n",
       " <Token index=4;words=[<Word index=4;text=学苑;lemma=学苑;upos=NOUN;xpos=NN;feats=_;governor=6;dependency_relation=nsubj>]>,\n",
       " <Token index=5;words=[<Word index=5;text=多;lemma=多;upos=NUM;xpos=CD;feats=NumType=Card;governor=6;dependency_relation=advmod>]>,\n",
       " <Token index=6;words=[<Word index=6;text=开;lemma=开;upos=VERB;xpos=VV;feats=_;governor=0;dependency_relation=root>]>,\n",
       " <Token index=7;words=[<Word index=7;text=设;lemma=设;upos=VERB;xpos=VV;feats=_;governor=6;dependency_relation=xcomp>]>,\n",
       " <Token index=8;words=[<Word index=8;text=有;lemma=有;upos=VERB;xpos=VV;feats=_;governor=7;dependency_relation=xcomp>]>,\n",
       " <Token index=9;words=[<Word index=9;text=书法;lemma=书法;upos=NOUN;xpos=NN;feats=_;governor=8;dependency_relation=obj>]>,\n",
       " <Token index=10;words=[<Word index=10;text=、;lemma=、;upos=PUNCT;xpos=EC;feats=_;governor=11;dependency_relation=punct>]>,\n",
       " <Token index=11;words=[<Word index=11;text=插花;lemma=插花;upos=NOUN;xpos=NN;feats=_;governor=9;dependency_relation=conj>]>,\n",
       " <Token index=12;words=[<Word index=12;text=、;lemma=、;upos=PUNCT;xpos=EC;feats=_;governor=14;dependency_relation=punct>]>,\n",
       " <Token index=13;words=[<Word index=13;text=土风;lemma=土风;upos=NOUN;xpos=NN;feats=_;governor=14;dependency_relation=nmod>]>,\n",
       " <Token index=14;words=[<Word index=14;text=舞班;lemma=舞班;upos=NOUN;xpos=NN;feats=_;governor=9;dependency_relation=conj>]>,\n",
       " <Token index=15;words=[<Word index=15;text=，;lemma=，;upos=PUNCT;xpos=.;feats=_;governor=6;dependency_relation=punct>]>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(lang='zh')\n",
    "\n",
    "text = \"\".join(raw_test[0])\n",
    "results = nlp(HanziConv.toSimplified(text))\n",
    "results.sentences[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['许多', '社区', '长青', '学苑', '多', '开', '设', '有', '书法', '、', '插花', '、', '土风', '舞班', '，']\n"
     ]
    }
   ],
   "source": [
    "toks = []\n",
    "for t in results.sentences[0].tokens:\n",
    "    toks.append(t.text)\n",
    "    \n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開', '設', '有', '書法', '、', '插花', '、', '土風', '舞班', '，']\n"
     ]
    }
   ],
   "source": [
    "print(restore(text, toks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
