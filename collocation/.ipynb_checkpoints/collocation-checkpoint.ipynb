{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The famous Brown corpus, built-in with NLTK, is used for exmaple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/hhhuang/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 tokens: \n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "The number of words in the Brown corpus: 1161192\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "# Get all words in the Brown Corpus.\n",
    "tokens = brown.words()\n",
    "print(\"The first 20 tokens: \")\n",
    "print(tokens[:20])\n",
    "\n",
    "print(\"The number of words in the Brown corpus: %d\" % len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup the Brown corpus at different linguistisc units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five sentences:\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'], ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.'], ['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']]\n",
      "Number of sentences: 57340\n",
      "\n",
      "\n",
      "The first five paragraphs:\n",
      "[[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']], [['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']], [['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']], [['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']], [['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']]]\n",
      "Number of paragraphs: 57340\n"
     ]
    }
   ],
   "source": [
    "print(\"The first five sentences:\")\n",
    "print(brown.sents()[0:5])\n",
    "print(\"Number of sentences: %d\" % len(brown.sents()))\n",
    "\n",
    "print(\"\\n\")\n",
    "# The Brown corpus does not provide the paragraph information. \n",
    "print(\"The first five paragraphs:\")\n",
    "print(brown.paras()[0:5])\n",
    "print(\"Number of paragraphs: %d\" % len(brown.sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged Tokens\n",
    "\n",
    "The Brown corpus also provides the words with part-of-speech tagged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 tokens with tag: \n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS')]\n"
     ]
    }
   ],
   "source": [
    "# Get all words in the Brown Corpus.\n",
    "print(\"The first 20 tokens with tag: \")\n",
    "print(brown.tagged_words()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental text processing (Last week)\n",
    "\n",
    "Alternatively, you can also perform the part-of-speech tagging by yourself following the procedure introduced last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence in the Brown corpus with POS tagged:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hhhuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hhhuang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('Fulton', 'NNP'), ('County', 'NNP'), ('Grand', 'NNP'), ('Jury', 'NNP'), ('said', 'VBD'), ('Friday', 'NNP'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NNP'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'IN'), ('any', 'DT'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"The first sentence in the Brown corpus with POS tagged:\")\n",
    "print(nltk.pos_tag(brown.sents()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t62713\n",
      ",\t58334\n",
      ".\t49346\n",
      "of\t36080\n",
      "and\t27915\n",
      "to\t25732\n",
      "a\t21881\n",
      "in\t19536\n",
      "that\t10237\n",
      "is\t10011\n",
      "was\t9777\n",
      "for\t8841\n",
      "``\t8837\n",
      "''\t8789\n",
      "The\t7258\n",
      "with\t7012\n",
      "it\t6723\n",
      "as\t6706\n",
      "he\t6566\n",
      "his\t6466\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(tokens)\n",
    "for w, c in word_counts.most_common(20):\n",
    "    print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Tokens excluding Punctuation Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t62713\n",
      "of\t36080\n",
      "and\t27915\n",
      "to\t25732\n",
      "a\t21881\n",
      "in\t19536\n",
      "that\t10237\n",
      "is\t10011\n",
      "was\t9777\n",
      "for\t8841\n",
      "The\t7258\n",
      "with\t7012\n",
      "it\t6723\n",
      "as\t6706\n",
      "he\t6566\n",
      "his\t6466\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(tokens)\n",
    "for w, c in word_counts.most_common(20):\n",
    "    if w.isalpha():\n",
    "        print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\tthe\t9625\n",
      "in\tthe\t5546\n",
      "to\tthe\t3426\n",
      "on\tthe\t2297\n",
      "and\tthe\t2136\n",
      "for\tthe\t1759\n",
      "to\tbe\t1697\n",
      "at\tthe\t1506\n",
      "with\tthe\t1472\n",
      "of\ta\t1461\n",
      "that\tthe\t1368\n",
      "from\tthe\t1351\n",
      "in\ta\t1316\n",
      "by\tthe\t1310\n",
      "as\ta\t896\n",
      "with\ta\t881\n",
      "it\tis\t881\n",
      "is\ta\t864\n",
      "of\this\t806\n",
      "is\tthe\t782\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "for i in range(len(tokens) - 1):\n",
    "    (w1, w2) = (tokens[i], tokens[i + 1])\n",
    "    if w1.isalpha() and w2.isalpha():\n",
    "        word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the content in word_pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 9625)\n"
     ]
    }
   ],
   "source": [
    "print(word_pair_counts.most_common(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9625\n"
     ]
    }
   ],
   "source": [
    "print(word_pair_counts[('of', 'the')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way to get the contents in the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\tthe\t9625\n",
      "in\tthe\t5546\n",
      "to\tthe\t3426\n",
      "on\tthe\t2297\n",
      "and\tthe\t2136\n",
      "for\tthe\t1759\n",
      "to\tbe\t1697\n",
      "at\tthe\t1506\n",
      "with\tthe\t1472\n",
      "of\ta\t1461\n",
      "that\tthe\t1368\n",
      "from\tthe\t1351\n",
      "in\ta\t1316\n",
      "by\tthe\t1310\n",
      "as\ta\t896\n",
      "with\ta\t881\n",
      "it\tis\t881\n",
      "is\ta\t864\n",
      "of\this\t806\n",
      "is\tthe\t782\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (w1, w2, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with POS Tag Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the POS tags in the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638), (',', 58156), ('NNS', 55110), ('CC', 37718), ('RB', 36464), ('NP', 34476), ('VB', 33693), ('VBN', 29186), ('VBD', 26167), ('CS', 22143), ('PPS', 18253), ('VBG', 17893), ('PP$', 16872), ('TO', 14918), ('PPSS', 13802), ('CD', 13510), ('NN-TL', 13372), ('MD', 12431), ('PPO', 11181), ('BEZ', 10066), ('BEDZ', 9806), ('AP', 9522), ('DT', 8957), ('``', 8837), (\"''\", 8789), ('QL', 8735), ('VBZ', 7373), ('BE', 6360), ('RP', 6009), ('WDT', 5539), ('HVD', 4895), ('*', 4603), ('WRB', 4509), ('BER', 4379), ('JJ-TL', 4107), ('NP-TL', 4019), ('HV', 3928), ('WPS', 3924), ('--', 3405), ('BED', 3282), ('ABN', 3010), ('DTI', 2921), ('PN', 2573), ('NP$', 2565), ('BEN', 2470), ('DTS', 2435), ('HVZ', 2433), (')', 2273), ('(', 2264), ('NNS-TL', 2226), ('EX', 2164), ('JJR', 1958), ('OD', 1935), ('NR', 1566), (':', 1558), ('NN$', 1480), ('IN-TL', 1477), ('NN-HL', 1471), ('DO', 1353), ('NPS', 1275), ('PPL', 1233), ('RBR', 1182), ('DOD', 1047), ('JJT', 1005), ('CD-TL', 898), ('MD*', 866), ('AT-TL', 746), ('ABX', 730), ('BEG', 686), ('NNS-HL', 609), ('UH', 608), ('.-HL', 598), ('VBN-TL', 591), ('NP-HL', 517), ('IN-HL', 508), ('DO*', 485), ('PPSS+MD', 484), ('DOZ', 467), ('CD-HL', 444), ('PPS+BEZ', 430), ('DOD*', 402), ('JJ-HL', 396), ('NN$-TL', 361), ('JJS', 359), ('ABL', 357), ('PPLS', 345), ('AT-HL', 332), (\"'\", 317), ('NR-TL', 309), ('CC-TL', 307), ('FW-NN', 288), ('HVG', 281), ('WPO', 280), ('PPSS+BER', 278), ('PPSS+BEM', 270), ('QLP', 261), ('NNS$', 257), ('WP$', 252), ('PPSS+HV', 241), ('HVN', 237), ('BEM', 226), ('OD-TL', 201), (')-HL', 184), ('DT+BEZ', 179), ('WQL', 176), (',-HL', 171), ('FW-NN-TL', 170), ('PP$$', 164), ('(-HL', 162), ('NIL', 157), ('BEDZ*', 154), ('VBG-HL', 146), ('PPS+MD', 144), ('NP$-TL', 141), (':-HL', 138), ('VBN-HL', 137), ('VBG-TL', 133), ('NN-TL-HL', 129), ('VB-HL', 125), ('CC-HL', 119), ('NN-NC', 118), ('BEZ*', 117), ('EX+BEZ', 105), ('DTX', 104), ('RBT', 101), ('HVD*', 99), ('VB-TL', 96), ('DOZ*', 89), ('PN$', 89), ('FW-IN', 84), ('PPSS+HVD', 83), ('FW-NNS', 83), ('PPS+HVD', 83), ('NNS$-TL', 74), ('FW-JJ-TL', 74), ('VBZ-HL', 72), ('VB+PPO', 71), ('NPS-TL', 67), ('NR$', 66), ('TO-HL', 55), ('FW-JJ', 53), ('RB-HL', 49), ('BER*', 47), ('WDT+BEZ', 47), ('FW-AT-TL', 44), ('PPS+HVZ', 43), ('HV*', 42), ('JJ-NC', 41), ('IN-NC', 41), ('VB-NC', 41), ('AP-HL', 40), ('RB-TL', 40), ('FW-IN-TL', 40), ('NPS$', 38), ('WRB-HL', 36), ('FW-NNS-TL', 36), ('PP$-TL', 35), ('AT-NC', 35), ('NN+BEZ', 34), ('FW-RB', 32), ('PPSS-NC', 31), ('BEZ-HL', 30), ('WDT-HL', 30), ('MD-HL', 27), ('FW-CC', 27), ('FW-VB', 26), ('JJ-TL-HL', 26), ('RB-NC', 26), ('---HL', 26), ('NNS-NC', 26), ('CS-HL', 25), ('NP+BEZ', 25), ('PPSS-HL', 25), ('FW-AT', 24), ('BED*', 22), ('HVZ*', 22), (':-TL', 22), ('WPS+BEZ', 21), ('JJS-TL', 20), ('NN$-HL', 20), ('PPS-HL', 19), ('AP-TL', 18), ('FW-IN+AT-TL', 18), ('JJR-HL', 17), ('VBZ-TL', 17), ('CD-TL-HL', 17), ('VBG+TO', 17), ('FW-WDT', 16), ('DOZ-HL', 16), ('NRS', 16), ('.-NC', 16), ('VBG-NC', 16), ('UH-TL', 15), ('JJR-TL', 15), ('NP-NC', 15), ('RP-HL', 14), ('NNS-TL-HL', 14), ('FW-CC-TL', 14), ('BE-HL', 13), ('PPO-TL', 13), ('FW-AT+NN-TL', 13), ('TO-NC', 13), ('PP$-NC', 13), ('WPS-TL', 12), ('FW-VBN', 12), ('BER-HL', 11), ('RB+BEZ', 11), ('NR$-TL', 11), ('WRB+BEZ', 11), ('HV-NC', 11), ('VBD-NC', 11), ('NR-HL', 10), ('TO-TL', 10), ('PP$-HL', 10), ('AP$', 9), ('RB$', 9), ('RN', 9), ('FW-PPL', 9), ('PPSS-TL', 9), ('DT-TL', 9), ('WRB-TL', 9), ('FW-NN$', 9), ('PPS-NC', 9), ('VBN-NC', 9), ('PPO-NC', 9), ('BEM*', 9), ('VBD-HL', 8), ('NPS-HL', 8), ('OD-HL', 8), ('MD-TL', 8), ('*-HL', 8), ('NP$-HL', 8), ('BEZ-TL', 8), ('WPS+MD', 8), ('FW-UH', 8), ('BEDZ-NC', 8), ('NP-TL-HL', 7), ('MD+HV', 7), ('FW-CD', 7), ('ABN-TL', 7), ('FW-NP', 7), ('FW-VBG', 7), ('DT-NC', 7), ('WRB-NC', 7), ('WDT-NC', 7), ('VBZ-NC', 7), ('PN+BEZ', 7), ('VBN-TL-HL', 6), ('DT-HL', 6), ('JJT-HL', 6), ('VBD-TL', 6), ('DTI-HL', 6), ('BER-TL', 6), ('QL-TL', 6), ('FW-*', 6), ('IN-TL-HL', 6), ('PPS-TL', 6), ('FW-NN-NC', 6), ('FW-PPSS', 6), ('WPS+HVD', 6), ('NP+HVZ', 6), ('WRB+DOD', 6), ('DT$', 5), ('FW-IN+NN', 5), ('CD$', 5), ('JJR-NC', 5), ('PPO-HL', 5), ('DO-TL', 5), ('WQL-TL', 5), ('PN-TL', 5), ('NR-TL-HL', 5), ('AT-TL-HL', 5), ('NN+HVZ', 5), ('VBN+TO', 5), ('BER-NC', 5), ('UH-NC', 5), (',-NC', 5), ('CD-NC', 5), ('RP-NC', 5), ('CC-NC', 5), ('CS-NC', 5), ('BEZ-NC', 5), ('ABN-HL', 4), ('DO-HL', 4), ('NNS$-HL', 4), ('WPO-TL', 4), ('FW-VBZ', 4), ('FW-PPO', 4), ('QL-HL', 4), ('FW-OD-TL', 4), ('HVZ-TL', 4), ('RP-TL', 4), (',-TL', 4), ('FW-NN$-TL', 4), ('FW-BEZ', 4), ('FW-NP-TL', 4), ('JJT-TL', 4), ('EX+MD', 4), ('FW-IN+AT', 4), ('NR-NC', 4), ('VB+TO', 4), ('RP+IN', 4), ('PN+HVZ', 3), ('FW-VB-NC', 3), ('NPS$-TL', 3), ('WRB+BEZ-TL', 3), ('FW-IN+AT-T', 3), ('FW-RB-TL', 3), ('HV-HL', 3), ('VB+IN', 3), ('DO*-HL', 3), ('FW-PP$', 3), ('FW-BER', 3), ('FW-NR-TL', 3), ('FW-CS', 3), ('HV-TL', 3), ('FW-PPO+IN', 3), ('VBN-TL-NC', 3), ('NNS-TL-NC', 3), ('NN-TL-NC', 3), ('BED-NC', 3), ('PPS+BEZ-NC', 3), ('NP+BEZ-NC', 3), ('WPS-NC', 3), ('EX+HVD', 3), ('PN+MD', 3), ('DT+MD', 3), ('HV+TO', 3), ('RB+CS', 3), ('FW-DT', 2), ('PN-HL', 2), ('FW-IN+NN-TL', 2), ('FW-AT+NP-TL', 2), ('BEN-TL', 2), ('CS-TL', 2), ('CC-TL-HL', 2), ('FW-JJ-NC', 2), ('FW-VBD', 2), ('FW-*-TL', 2), ('DTS-HL', 2), ('PN-NC', 2), ('WDT+HVZ', 2), ('FW-IN+NP-TL', 2), ('NN+MD', 2), ('FW-NNS-NC', 2), ('VB+RP', 2), ('FW-PP$-TL', 2), ('DTI-TL', 2), ('.-TL', 2), ('FW-NPS', 2), ('FW-CD-TL', 2), ('FW-PPL+VBZ', 2), ('DOZ-TL', 2), ('WDT+BEZ-NC', 2), ('HVZ-NC', 2), ('QL-NC', 2), ('WPS+BEZ-NC', 2), ('PPSS+MD-NC', 2), ('AP-NC', 2), ('DO-NC', 2), ('MD-NC', 2), ('NNS$-NC', 2), ('PPL-NC', 2), ('BEM-NC', 2), ('NPS-NC', 2), ('JJ+JJ-NC', 2), ('WPS-HL', 2), ('FW-DT+BEZ', 2), ('WPS+HVZ', 2), ('MD+TO', 2), ('NN+BEZ-TL', 2), ('EX+HVZ', 2), ('PPSS+VB', 2), ('NNS+MD', 2), ('NP+MD', 2), ('TO+VB', 2), ('VB+AT', 2), ('DTS+BEZ', 2), ('MD*-HL', 1), ('BEDZ-HL', 1), ('PPS+BEZ-HL', 1), ('HVD-HL', 1), ('FW-AT-HL', 1), ('FW-PP$-NC', 1), ('NPS$-HL', 1), ('UH-HL', 1), ('WDT+BEZ-HL', 1), ('PPL-HL', 1), ('FW-VBD-TL', 1), ('PPSS+BER-TL', 1), ('BE-TL', 1), ('PPSS+HV-TL', 1), ('DOD*-TL', 1), ('WDT+BEZ-TL', 1), ('FW-JJR', 1), ('WDT+BER+PP', 1), ('FW-UH-NC', 1), ('RB+BEZ-HL', 1), ('JJS-HL', 1), ('PPL-TL', 1), ('JJR+CS', 1), ('NRS-TL', 1), ('FW-HV', 1), ('DOZ*-TL', 1), ('FW-NPS-TL', 1), ('*-TL', 1), ('FW-PN', 1), ('FW-BE', 1), ('FW-PPS', 1), ('FW-NR', 1), ('FW-TO+VB', 1), ('JJ$-TL', 1), ('FW-VB-TL', 1), ('FW-RB+CC', 1), ('FW-WPO', 1), ('FW-NN-TL-NC', 1), ('FW-WPS', 1), ('FW-DTS', 1), ('NNS$-TL-HL', 1), ('FW-VBG-TL', 1), ('EX-HL', 1), ('PPSS+BER-N', 1), ('NP+HVZ-NC', 1), ('DT+BEZ-NC', 1), ('RB+BEZ-NC', 1), ('*-NC', 1), ('EX-NC', 1), ('BER*-NC', 1), ('PPSS+BER-NC', 1), ('RBR-NC', 1), ('OD-NC', 1), ('ABN-NC', 1), ('JJT-NC', 1), ('DOD-NC', 1), ('WPO-NC', 1), ('NN+NN-NC', 1), ('AP+AP-NC', 1), ('VB+JJ-NC', 1), ('VB+VB-NC', 1), ('FW-QL', 1), ('JJ-TL-NC', 1), ('FW-JJT', 1), ('WPS+BEZ-TL', 1), ('HVG-HL', 1), ('MD+PPSS', 1), ('NR+MD', 1), ('NN+IN', 1), ('NN+HVD-TL', 1), ('WDT+DOD', 1), ('WRB+DO', 1), ('WRB+IN', 1), ('WRB+MD', 1), ('NN+HVZ-TL', 1), ('WRB+BER', 1), ('PPSS+BEZ', 1), ('PPSS+BEZ*', 1), ('RBR+CS', 1), ('IN+PPO', 1), ('IN+IN', 1), ('DO+PPSS', 1), ('WRB+DOZ', 1), ('WDT+DO+PPS', 1), ('WRB+DOD*', 1), ('WDT+BER', 1), ('FW-OD-NC', 1), ('FW-PPSS+HV', 1), ('PN+HVD', 1), ('FW-UH-TL', 1)]\n"
     ]
    }
   ],
   "source": [
    "tag_counter = Counter()\n",
    "tagged_tokens = brown.tagged_words()\n",
    "for (word, tag) in tagged_tokens:\n",
    "    tag_counter[tag] += 1\n",
    "    \n",
    "print(tag_counter.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the results with the pattern A + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiscal\tyear\t56\n",
      "high\tschool\t54\n",
      "old\tman\t52\n",
      "young\tman\t47\n",
      "great\tdeal\t43\n",
      "long\ttime\t39\n",
      "new\tmembers\t30\n",
      "young\tmen\t29\n",
      "dominant\tstress\t28\n",
      "real\testate\t27\n",
      "foreign\tpolicy\t27\n",
      "good\tdeal\t27\n",
      "recent\tyears\t25\n",
      "large\tnumber\t25\n",
      "small\tbusiness\t24\n",
      "American\tpeople\t22\n",
      "human\tbeings\t22\n",
      "nuclear\tweapons\t21\n",
      "front\tdoor\t20\n",
      "big\tman\t17\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if tag1 == 'JJ' and tag2[0] == 'N':\n",
    "        word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the resuls with the pattern N + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhode\tIsland\t90\n",
      "World\tWar\t60\n",
      "U.\tS.\t57\n",
      "Peace\tCorps\t52\n",
      "Los\tAngeles\t47\n",
      "President\tKennedy\t40\n",
      "General\tMotors\t40\n",
      "San\tFrancisco\t39\n",
      "Mr.\tKennedy\t34\n",
      "Du\tPont\t34\n",
      "St.\tLouis\t32\n",
      "St.\tJohn\t28\n",
      "Soviet\tUnion\t27\n",
      "Sam\tRayburn\t26\n",
      "Air\tForce\t26\n",
      "York\tCity\t25\n",
      "home\truns\t25\n",
      "State\tDepartment\t24\n",
      "Kansas\tCity\t24\n",
      "Jesus\tChrist\t24\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if tag1[0] == 'N' and tag2[0] == 'N':\n",
    "        word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the resuls with the pattern N + N + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug's\tchemical\tname\t15\n",
      "John\tA.\tNotte\t15\n",
      "General\tMotors\tstock\t10\n",
      "Prairie\tDu\tChien\t8\n",
      "hearing\tofficer's\treport\t8\n",
      "John\tF.\tKennedy\t7\n",
      "tax\tcollection\tyear\t7\n",
      "combustion\tchamber\tvolume\t6\n",
      "Lauro\tDi\tBosis\t6\n",
      "labor\tsurplus\tareas\t6\n",
      "Mr.\tJustice\tFrankfurter\t6\n",
      "home\trule\tcharter\t5\n",
      "Lord\tJesus\tChrist\t5\n",
      "Peace\tCorps\tvolunteers\t5\n",
      "Field\tMarshal\tSlim\t5\n",
      "Hudson's\tBay\tCompany\t5\n",
      "Dwight\tD.\tEisenhower\t4\n",
      "Mrs.\tWilliam\tH.\t4\n",
      "potato\tchip\tindustry\t4\n",
      "Wall\tStreet\tJournal\t4\n"
     ]
    }
   ],
   "source": [
    "word_trio_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 2):\n",
    "    (w1, w2, w3) = (tagged_tokens[i][0], tagged_tokens[i + 1][0], tagged_tokens[i + 2][0])\n",
    "    (tag1, tag2, tag3) = (tagged_tokens[i][1], tagged_tokens[i + 1][1], tagged_tokens[i + 2][1])\n",
    "    if tag1[0] == 'N' and tag2[0] == 'N' and tag3[0] == 'N':\n",
    "        word_trio_counts[(w1, w2, w3)] += 1\n",
    "    \n",
    "for pair, c in word_trio_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%s\\t%d\" % (pair[0], pair[1], pair[2], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Bigram Collocations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhode\tIsland\t90\n",
      "World\tWar\t60\n",
      "U.\tS.\t57\n",
      "fiscal\tyear\t56\n",
      "high\tschool\t54\n",
      "Peace\tCorps\t52\n",
      "old\tman\t52\n",
      "young\tman\t47\n",
      "Los\tAngeles\t47\n",
      "great\tdeal\t43\n",
      "President\tKennedy\t40\n",
      "General\tMotors\t40\n",
      "long\ttime\t39\n",
      "San\tFrancisco\t39\n",
      "Mr.\tKennedy\t34\n"
     ]
    }
   ],
   "source": [
    "def match_patterns(tag1, tag2):\n",
    "    if tag1[0] == 'N' and tag2[0] == 'N':\n",
    "        return True\n",
    "    elif tag1 == 'JJ' and tag2[0] == 'N':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if match_patterns(tag1, tag2):\n",
    "        word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(15):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Trigram Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way\tof\tlife\t28\n",
      "point\tof\tview\t26\n",
      "time\tto\ttime\t24\n",
      "period\tof\ttime\t20\n",
      "matter\tof\tfact\t17\n",
      "basic\twage\trate\t16\n",
      "Drug's\tchemical\tname\t15\n",
      "John\tA.\tNotte\t15\n",
      "number\tof\tpeople\t13\n",
      "years\tof\tage\t12\n",
      "number\tof\tyears\t12\n",
      "small\tbusiness\tconcerns\t12\n",
      "couple\tof\tweeks\t11\n",
      "uniform\tfiscal\tyear\t11\n",
      "side\tby\tside\t10\n"
     ]
    }
   ],
   "source": [
    "def match_patterns(tag1, tag2, tag3):\n",
    "    if tag1 == 'JJ' and tag2 == 'JJ' and tag3[0] == 'N':\n",
    "        return True\n",
    "    elif tag1 == 'JJ' and tag2[0] == 'N' and tag3[0] == 'N':\n",
    "        return True\n",
    "    elif tag1[0] == 'N' and tag2 == 'JJ' and tag3[0] == 'N':\n",
    "        return True\n",
    "    elif tag1[0] == 'N' and tag2[0] == 'N' and tag3[0] == 'N':\n",
    "        return True\n",
    "    elif tag1[0] == 'N' and tag2 == 'IN' and tag3[0] == 'N':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "word_trio_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 2):\n",
    "    (w1, w2, w3) = (tagged_tokens[i][0], tagged_tokens[i + 1][0], tagged_tokens[i + 2][0])\n",
    "    (tag1, tag2, tag3) = (tagged_tokens[i][1], tagged_tokens[i + 1][1], tagged_tokens[i + 2][1])\n",
    "    if match_patterns(tag1, tag2, tag3):\n",
    "        word_trio_counts[(w1, w2, w3)] += 1\n",
    "    \n",
    "for pair, c in word_trio_counts.most_common(15):\n",
    "    print(\"%s\\t%s\\t%s\\t%d\" % (pair[0], pair[1], pair[2], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations in the pattern powerful + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powerful\tengines\t2\n",
      "powerful\tarms\t2\n",
      "powerful\ttransmitter\t1\n",
      "powerful\tman\t1\n",
      "powerful\tnations\t1\n",
      "powerful\tnation\t1\n",
      "powerful\tmirror\t1\n",
      "powerful\tefforts\t1\n",
      "powerful\tweapon\t1\n",
      "powerful\tglasses\t1\n",
      "powerful\tdivine\t1\n",
      "powerful\tvictory\t1\n",
      "powerful\tmusic\t1\n",
      "powerful\tact\t1\n",
      "powerful\tindictment\t1\n",
      "powerful\tcongressmen\t1\n",
      "powerful\tinfluence\t1\n",
      "powerful\topposition\t1\n",
      "powerful\torwell\t1\n",
      "powerful\tfactor\t1\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "\n",
    "powerful_partners = set()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if w1.lower() == 'powerful' and tag2[0] == 'N':\n",
    "        word_pair_counts[(w1.lower(), w2.lower())] += 1\n",
    "        powerful_partners.add(w2.lower())\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations in the pattern strong + N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strong\tstress\t11\n",
      "strong\thands\t7\n",
      "strong\topposition\t3\n",
      "strong\tmen\t3\n",
      "strong\tarm\t2\n",
      "strong\tpressures\t2\n",
      "strong\toil\t2\n",
      "strong\tpoint\t2\n",
      "strong\tfeeling\t2\n",
      "strong\twoman\t2\n",
      "strong\tstresses\t2\n",
      "strong\tencouragement\t1\n",
      "strong\tfight\t1\n",
      "strong\tadvocate\t1\n",
      "strong\texecutives\t1\n",
      "strong\tchristianity\t1\n",
      "strong\treactions\t1\n",
      "strong\tpressure\t1\n",
      "strong\tdissents\t1\n",
      "strong\tconviction\t1\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "\n",
    "strong_partners = set()\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if w1.lower() == 'strong' and tag2[0] == 'N':\n",
    "        word_pair_counts[(w1.lower(), w2.lower())] += 1\n",
    "        strong_partners.add(w2.lower())\n",
    "    \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words only occur with powerful:\n",
      "['transmitter', 'representative', 'force', 'bond', 'nations', 'act', 'glasses', 'factor', 'efforts', 'union', 'music', 'indictment', 'congressmen', 'sources', 'microphone', 'victory', 'orwell', 'jab', 'societies', 'nation', 'man', 'weapon', 'civilizations', 'introject', 'streams', 'aid', 'mirror', 'arms', 'engines', 'body', 'divine']\n",
      "The words only occur with strong:\n",
      "['af', 'compulsion', 'dissents', 'explanation', 'temperance', 'listener', 'suspicions', 'fight', 'potions', 'pressures', 'features', 'light', 'convictions', 'nose', 'executives', 'home-blend', 'hints', 'stresses', 'position', 'jaws', 'liking', 'feeling', 'support', 'delights', 'winds', 'hand', 'men', 'reactions', 'determination', 'activity', 'stress', 'emotion', 'recovery', 'possibility', 'signals', 'demand', 'poland', 'tea', 'wave', 'wine', 'material', 'backing', 'adhesive', 'credit', 'contrast', 'point', 'performance', 'arm', 'transom', 'protest', 'branches', 'oil', 'apple', 'christianity', 'beliefs', '20th-century', 'incentives', 'pressure', 'push', 'look', 'case', 'conviction', 'voice', 'encouragement', 'emotions', 'subordinates', 'upsurge', 'word', 'woman', 'advocate', 'tone', 'opinions']\n",
      "The words occur with both strong and powerful:\n",
      "['influence', 'bulwark', 'hands', 'opposition', 'sense']\n"
     ]
    }
   ],
   "source": [
    "print(\"The words only occur with powerful:\")\n",
    "print(list(powerful_partners - strong_partners))\n",
    "\n",
    "print(\"The words only occur with strong:\")\n",
    "print(list(strong_partners - powerful_partners))\n",
    "\n",
    "print(\"The words occur with both strong and powerful:\")\n",
    "print(list(strong_partners & powerful_partners))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went\tto\t109\n",
      "go\tto\t96\n",
      "came\tto\t96\n",
      "used\tin\t93\n",
      "based\ton\t82\n",
      "looked\tat\t81\n",
      "found\tin\t77\n",
      "come\tto\t72\n",
      "think\tof\t72\n",
      "look\tat\t70\n",
      "shown\tin\t62\n",
      "interested\tin\t61\n",
      "related\tto\t59\n",
      "thought\tof\t59\n",
      "made\tby\t56\n",
      "made\tin\t56\n",
      "followed\tby\t56\n",
      "live\tin\t54\n",
      "returned\tto\t54\n",
      "concerned\twith\t52\n"
     ]
    }
   ],
   "source": [
    "word_pair_counts = Counter()\n",
    "\n",
    "for i in range(len(tagged_tokens) - 1):\n",
    "    (w1, w2) = (tagged_tokens[i][0], tagged_tokens[i + 1][0])\n",
    "    (tag1, tag2) = (tagged_tokens[i][1], tagged_tokens[i + 1][1])\n",
    "    if tag1[0] == 'V' and tag2 == 'IN':\n",
    "        word_pair_counts[(w1.lower(), w2.lower())] += 1\n",
    "        \n",
    "for pair, c in word_pair_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\" % (pair[0], pair[1], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distant Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent collocations with a distance of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n",
      "the\tof\t2\t10822\n",
      "of\tthe\t1\t9717\n",
      "the\tthe\t3\t6529\n",
      "in\tthe\t1\t6025\n",
      "the\tthe\t4\t5648\n",
      "the\tthe\t8\t4984\n",
      "the\tthe\t7\t4955\n",
      "the\tthe\t6\t4806\n",
      "the\tthe\t5\t4346\n",
      "the\tof\t3\t4304\n",
      "to\tthe\t1\t3484\n",
      "a\tof\t2\t2806\n",
      "of\tthe\t4\t2510\n",
      "of\tthe\t5\t2506\n",
      "the\tand\t2\t2468\n",
      "on\tthe\t1\t2466\n",
      "the\tof\t8\t2430\n",
      "of\tthe\t8\t2379\n",
      "of\tthe\t6\t2376\n",
      "of\tthe\t7\t2370\n"
     ]
    }
   ],
   "source": [
    "window_size = 9\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "word_pair_distance_counts = Counter()\n",
    "for i in range(len(tokens) - 1):\n",
    "    w1 = tokens[i]\n",
    "    if not w1.isalpha():\n",
    "        continue\n",
    "    for distance in range(1, window_size):\n",
    "        if i + distance < len(tokens):\n",
    "            w2 = tokens[i + distance]\n",
    "            if not w2.isalpha():\n",
    "                continue\n",
    "            word_pair_distance_counts[(w1.lower(), w2.lower(), distance)] += 1\n",
    "            word_pair_counts[(w1.lower(), w2.lower())] += 1\n",
    "print(len(tokens))\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%d\" % (w1, w2, distance, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an entry in word_pair_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('the', 'of', 2), 10822)\n",
      "0\n",
      "0\n",
      "Occurrences of the word pair (the, of) with a distance of 1: 0\n",
      "Occurrences of the word pair (the, of) with a distance of 2: 10822\n",
      "Occurrences of the word pair (the, of) with a distance of 3: 4304\n",
      "Occurrences of the word pair (the, of) with a distance of 4: 1662\n",
      "Occurrences of the word pair (the, of) with a distance of 5: 2080\n",
      "Occurrences of the word pair (the, of) with a distance of 6: 2101\n",
      "Occurrences of the word pair (the, of) with a distance of 7: 2123\n",
      "Occurrences of the word pair (the, of) with a distance of 8: 2430\n",
      "Occurrences of the usage 'the * * of'\n",
      "10822\n",
      "Occurrences of the usage 'of * * the'\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "print(word_pair_distance_counts.most_common(1)[0])\n",
    "\n",
    "print(word_pair_distance_counts['the', 'of', 1])\n",
    "print(word_pair_distance_counts['the', 'of', 100])\n",
    "\n",
    "\n",
    "for distance in range(1, window_size):\n",
    "    print(\"Occurrences of the word pair (%s, %s) with a distance of %d: %d\" % (\n",
    "        'the', 'of', distance, word_pair_distance_counts['the', 'of', distance]))\n",
    "\n",
    "print(\"Occurrences of the usage 'the * * of'\")\n",
    "print(word_pair_distance_counts['the', 'of', 2])\n",
    "\n",
    "print(\"Occurrences of the usage 'of * * the'\")\n",
    "print(word_pair_distance_counts['of', 'the', 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the collocations with mean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tgovernor\t8.000000\t7\n",
      "governor\tprovidence\t8.000000\t7\n",
      "state\tproclaim\t8.000000\t7\n",
      "testimony\thand\t8.000000\t7\n",
      "whereof\tand\t8.000000\t7\n",
      "have\tseal\t8.000000\t7\n",
      "hereunto\tof\t8.000000\t7\n",
      "caused\taffixed\t8.000000\t7\n",
      "proclamation\tgovernor\t8.000000\t6\n",
      "independence\tgovernor\t8.000000\t6\n",
      "paragraphs\tc\t8.000000\t6\n",
      "a\tmachines\t8.000000\t5\n",
      "made\tsaw\t8.000000\t5\n",
      "guam\tto\t8.000000\t5\n",
      "pay\twould\t8.000000\t4\n",
      "carcass\tthe\t8.000000\t4\n",
      "he\tmachines\t8.000000\t4\n",
      "controversial\tthe\t8.000000\t4\n",
      "populated\tto\t8.000000\t4\n",
      "talked\tthat\t8.000000\t4\n"
     ]
    }
   ],
   "source": [
    "pair_mean_distances = Counter()\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "for (w1, w2), distance in pair_mean_distances.most_common(20):\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering one-time cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2532836\n",
      "i\tgovernor\t8.000000\t7\n",
      "governor\tprovidence\t8.000000\t7\n",
      "state\tproclaim\t8.000000\t7\n",
      "testimony\thand\t8.000000\t7\n",
      "whereof\tand\t8.000000\t7\n",
      "have\tseal\t8.000000\t7\n",
      "hereunto\tof\t8.000000\t7\n",
      "caused\taffixed\t8.000000\t7\n",
      "proclamation\tgovernor\t8.000000\t6\n",
      "independence\tgovernor\t8.000000\t6\n",
      "paragraphs\tc\t8.000000\t6\n",
      "thousand\tindependence\t7.714286\t7\n",
      "by\tdiscovered\t7.666667\t6\n",
      "of\tconstants\t7.666667\t6\n",
      "court\tfrom\t7.666667\t6\n",
      "should\twere\t7.666667\t6\n",
      "at\tpaper\t7.666667\t6\n",
      "more\tvery\t7.666667\t6\n",
      "were\tshould\t7.666667\t6\n",
      "going\twere\t7.571429\t7\n"
     ]
    }
   ],
   "source": [
    "pair_mean_distances = Counter()\n",
    "\n",
    "print(len(word_pair_counts))\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 5:\n",
    "        pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "for (w1, w2), distance in pair_mean_distances.most_common(20):\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image\tintensifiers\t1.000000\t6\n",
      "electron\toptical\t1.000000\t6\n",
      "drove\thome\t1.000000\t6\n",
      "hour\tlater\t1.000000\t6\n",
      "go\tahead\t1.000000\t6\n",
      "damn\tit\t1.000000\t6\n",
      "kent\thouse\t1.000000\t6\n",
      "jean\tjacques\t1.000000\t6\n",
      "rector\tsaid\t1.000000\t6\n",
      "backed\toff\t1.000000\t6\n",
      "his\tcheek\t1.000000\t6\n",
      "shell\tpeople\t1.000000\t6\n",
      "brannon\tsaid\t1.000000\t6\n",
      "tom\thorn\t1.000000\t6\n",
      "benson\tsaid\t1.000000\t6\n",
      "herr\tschaffner\t1.000000\t6\n",
      "miss\tjen\t1.000000\t6\n",
      "gratt\tshafer\t1.000000\t6\n",
      "eddie\tlee\t1.000000\t6\n",
      "hanford\tcollege\t1.000000\t6\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), distance in pair_mean_distances.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by\tfaculty\t4.454545\t11\n",
      "who\tover\t4.454545\t11\n",
      "then\thas\t4.454545\t11\n",
      "diet\tand\t4.454545\t11\n",
      "have\tfind\t4.454545\t11\n",
      "children\ti\t4.454545\t11\n",
      "am\tmy\t4.454545\t11\n",
      "china\tin\t4.454545\t11\n",
      "than\tused\t4.454545\t11\n",
      "and\thearts\t4.454545\t11\n",
      "you\tyet\t4.454545\t11\n",
      "well\tyour\t4.454545\t11\n",
      "and\toperating\t4.454545\t11\n",
      "and\tdepartments\t4.454545\t11\n",
      "a\tstands\t4.454545\t11\n",
      "and\tdiscover\t4.454545\t11\n",
      "testimony\tof\t4.454545\t11\n",
      "better\tthere\t4.454545\t11\n",
      "for\tact\t4.454545\t11\n",
      "relax\tthe\t4.454545\t11\n"
     ]
    }
   ],
   "source": [
    "num_pairs = len(pair_mean_distances)\n",
    "mid = num_pairs // 2\n",
    "for (w1, w2), distance in pair_mean_distances.most_common()[mid-10:mid+10]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering with offset deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\tdenver\t4.500000\t3.834058\t6\n",
      "very\tcenter\t4.500000\t3.834058\t6\n",
      "may\tnever\t4.500000\t3.834058\t6\n",
      "be\tstarted\t4.000000\t3.741657\t7\n",
      "his\tobjective\t4.000000\t3.741657\t7\n",
      "and\tbelly\t5.000000\t3.741657\t7\n",
      "more\treason\t4.000000\t3.741657\t7\n",
      "placing\ta\t4.000000\t3.741657\t7\n",
      "year\tbecause\t4.333333\t3.669696\t6\n",
      "engineer\tfor\t4.333333\t3.669696\t6\n",
      "mechanism\tis\t4.333333\t3.669696\t6\n",
      "frequently\thas\t4.333333\t3.669696\t6\n",
      "and\texpectations\t4.333333\t3.669696\t6\n",
      "to\ttie\t4.333333\t3.669696\t6\n",
      "a\twider\t4.333333\t3.669696\t6\n",
      "policy\twill\t4.666667\t3.669696\t6\n",
      "devices\tto\t4.333333\t3.669696\t6\n",
      "the\tfierce\t4.333333\t3.669696\t6\n",
      "in\tapplying\t4.333333\t3.669696\t6\n",
      "operating\tpolicy\t4.333333\t3.669696\t6\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 5:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common(20):\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour\tlater\t1.000000\t0.000000\t6\n",
      "go\tahead\t1.000000\t0.000000\t6\n",
      "damn\tit\t1.000000\t0.000000\t6\n",
      "kent\thouse\t1.000000\t0.000000\t6\n",
      "got\tfeet\t3.000000\t0.000000\t6\n",
      "jean\tjacques\t1.000000\t0.000000\t6\n",
      "rector\tsaid\t1.000000\t0.000000\t6\n",
      "backed\toff\t1.000000\t0.000000\t6\n",
      "make\tfeel\t2.000000\t0.000000\t6\n",
      "his\tcheek\t1.000000\t0.000000\t6\n",
      "to\tjeep\t2.000000\t0.000000\t6\n",
      "shell\tpeople\t1.000000\t0.000000\t6\n",
      "brannon\tsaid\t1.000000\t0.000000\t6\n",
      "tom\thorn\t1.000000\t0.000000\t6\n",
      "benson\tsaid\t1.000000\t0.000000\t6\n",
      "herr\tschaffner\t1.000000\t0.000000\t6\n",
      "miss\tjen\t1.000000\t0.000000\t6\n",
      "gratt\tshafer\t1.000000\t0.000000\t6\n",
      "eddie\tlee\t1.000000\t0.000000\t6\n",
      "hanford\tcollege\t1.000000\t0.000000\t6\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher supportive threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall\tstreet\t1.000000\t0.000000\t11\n",
      "western\teurope\t1.000000\t0.000000\t11\n",
      "great\tbritain\t1.000000\t0.000000\t11\n",
      "status\tquo\t1.000000\t0.000000\t11\n",
      "taken\tplace\t1.000000\t0.000000\t11\n",
      "declaration\tindependence\t2.000000\t0.000000\t11\n",
      "strong\tenough\t1.000000\t0.000000\t11\n",
      "fat\tman\t1.000000\t0.000000\t11\n",
      "word\tgod\t2.000000\t0.000000\t11\n",
      "room\ttemperature\t1.000000\t0.000000\t11\n",
      "ballistic\tmissile\t1.000000\t0.000000\t11\n",
      "be\tmaintained\t1.000000\t0.000000\t11\n",
      "middle\tages\t1.000000\t0.000000\t11\n",
      "good\tluck\t1.000000\t0.000000\t11\n",
      "less\tdeveloped\t1.000000\t0.000000\t11\n",
      "motor\tpool\t1.000000\t0.000000\t11\n",
      "strong\tstress\t1.000000\t0.000000\t11\n",
      "in\toperand\t2.000000\t0.000000\t11\n",
      "oxygen\ttransfer\t1.000000\t0.000000\t11\n",
      "blue\tthroat\t1.000000\t0.000000\t11\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 10:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_counts = Counter()\n",
    "word_counts = Counter(tokens)\n",
    "num_bigrams = 0\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    w1 = tokens[i]\n",
    "    w2 = tokens[i + 1]\n",
    "    if w1.isalpha() and w2.isalpha():\n",
    "        word_pair_counts[(w1, w2)] += 1\n",
    "        num_bigrams += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-Square function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquare(o11, o12, o21, o22):\n",
    "    n = o11 + o12 + o21 + o22\n",
    "    x_2 = (n * ((o11 * o22 - o12 * o21)**2)) / ((o11 + o12) * (o11 + o21) * (o12 + o22) * (o21 + o22)) \n",
    "    return x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the chi-squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo\tShu\t21\t838076.000000\n",
      "Hong\tKong\t11\t838076.000000\n",
      "deja\tvue\t7\t838076.000000\n",
      "Notre\tDame\t6\t838076.000000\n",
      "Baton\tRouge\t5\t838076.000000\n",
      "Dolce\tVita\t5\t838076.000000\n",
      "Duncan\tPhyfe\t4\t838076.000000\n",
      "Hwang\tPah\t4\t838076.000000\n",
      "Chemische\tKrystallographie\t4\t838076.000000\n",
      "Beech\tPasture\t4\t838076.000000\n",
      "Ku\tKlux\t3\t838076.000000\n",
      "Klux\tKlan\t3\t838076.000000\n",
      "Sancho\tPanza\t3\t838076.000000\n",
      "bel\tcanto\t3\t838076.000000\n",
      "Estate\tBoards\t3\t838076.000000\n",
      "Sultan\tAhmet\t3\t838076.000000\n",
      "Planned\tParenthood\t3\t838076.000000\n",
      "Furious\tOverfall\t3\t838076.000000\n",
      "Grands\tCrus\t3\t838076.000000\n",
      "Agreeable\tAutocracies\t3\t838076.000000\n"
     ]
    }
   ],
   "source": [
    "pair_chi_squares = Counter()\n",
    "for (w1, w2), w1_w2_count in word_pair_counts.most_common():\n",
    "    w1_only_count = word_counts[w1] - w1_w2_count\n",
    "    w2_only_count = word_counts[w2] - w1_w2_count\n",
    "    rest_count = num_bigrams - w1_only_count - w2_only_count - w1_w2_count\n",
    "    pair_chi_squares[(w1, w2)] = chisquare(w1_w2_count, w1_only_count, w2_only_count, rest_count)\n",
    "\n",
    "for (w1, w2), x_2 in pair_chi_squares.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%f\" % (w1, w2, word_pair_counts[(w1, w2)], x_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on more frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo\tShu\t21\t838076.000000\n",
      "Hong\tKong\t11\t838076.000000\n",
      "Los\tAngeles\t47\t772340.862538\n",
      "Puerto\tRico\t21\t733313.874934\n",
      "Viet\tNam\t13\t680934.312462\n",
      "United\tStates\t392\t624942.825079\n",
      "Simms\tPurdew\t12\t591579.529361\n",
      "Armed\tForces\t22\t518035.039321\n",
      "carbon\ttetrachloride\t18\t510399.811865\n",
      "Air\tForce\t26\t484209.555146\n",
      "Rhode\tIsland\t90\t478857.709261\n",
      "San\tFrancisco\t39\t444131.903948\n",
      "New\tYork\t296\t422705.994262\n",
      "Peace\tCorps\t52\t415016.950298\n",
      "Du\tPont\t34\t413123.371544\n",
      "Saxon\tShore\t12\t352865.684075\n",
      "per\tcent\t146\t311401.424614\n",
      "Virgin\tIslands\t13\t302628.444270\n",
      "minimal\tpolynomial\t16\t283779.555274\n",
      "Linda\tKay\t17\t274594.311495\n"
     ]
    }
   ],
   "source": [
    "pair_chi_squares = Counter()\n",
    "for (w1, w2), w1_w2_count in word_pair_counts.most_common():\n",
    "    if w1_w2_count > 10:\n",
    "        w1_only_count = word_counts[w1] - w1_w2_count\n",
    "        w2_only_count = word_counts[w2] - w1_w2_count\n",
    "        rest_count = num_bigrams - w1_only_count - w2_only_count - w1_w2_count\n",
    "        pair_chi_squares[(w1, w2)] = chisquare(w1_w2_count, w1_only_count, w2_only_count, rest_count)\n",
    "\n",
    "for (w1, w2), x_2 in pair_chi_squares.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%f\" % (w1, w2, word_pair_counts[(w1, w2)], x_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\ta\t15\t0.003443\n",
      "is\tlike\t15\t0.003435\n",
      "use\tthe\t42\t0.003194\n",
      "those\tin\t18\t0.002944\n",
      "in\tuse\t13\t0.002915\n",
      "all\tI\t17\t0.002725\n",
      "the\tmind\t24\t0.002673\n",
      "to\tthese\t38\t0.002399\n",
      "the\tfine\t11\t0.002171\n",
      "city\tof\t11\t0.002115\n",
      "business\tto\t11\t0.001959\n",
      "got\tthe\t35\t0.001839\n",
      "feel\tthe\t16\t0.001782\n",
      "not\tone\t15\t0.001756\n",
      "nor\tthe\t12\t0.000947\n",
      "work\tand\t25\t0.000900\n",
      "that\tother\t20\t0.000815\n",
      "right\tin\t14\t0.000515\n",
      "to\tJohn\t11\t0.000265\n",
      "brought\tthe\t19\t0.000265\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), x_2 in pair_chi_squares.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%d\\t%f\" % (w1, w2, word_pair_counts[(w1, w2)], x_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top collocations with a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United\tStates\t392\n",
      "New\tYork\t296\n",
      "per\tcent\t146\n",
      "years\tago\t136\n",
      "Rhode\tIsland\t90\n",
      "White\tHouse\t65\n",
      "World\tWar\t60\n",
      "Peace\tCorps\t52\n",
      "United\tNations\t49\n",
      "Los\tAngeles\t47\n",
      "General\tMotors\t41\n",
      "New\tOrleans\t40\n",
      "San\tFrancisco\t39\n",
      "Civil\tWar\t36\n",
      "Du\tPont\t34\n",
      "nineteenth\tcentury\t30\n",
      "dominant\tstress\t28\n",
      "Supreme\tCourt\t27\n",
      "real\testate\t27\n",
      "Air\tForce\t26\n"
     ]
    }
   ],
   "source": [
    "rank = Counter()\n",
    "\n",
    "for (w1, w2), x_2 in pair_chi_squares.most_common():\n",
    "    if x_2 > 50000:\n",
    "        rank[(w1, w2)] = word_pair_counts[(w1, w2)]\n",
    "for (w1, w2), c in rank.most_common(20):        \n",
    "    print(\"%s\\t%s\\t%d\" % (w1, w2, c))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for computing mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mutual_information(w1_w2_prob, w1_prob, w2_prob):\n",
    "    return math.log2(w1_w2_prob / (w1_prob * w2_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durwood\tPye\t1\t20.617629\n",
      "Decries\tjoblessness\t1\t20.617629\n",
      "Resentment\twelled\t1\t20.617629\n",
      "Audrey\tKnecht\t1\t20.617629\n",
      "Frankford\tElevated\t1\t20.617629\n",
      "vouchers\tcertifying\t1\t20.617629\n",
      "Inspections\tBarnet\t1\t20.617629\n",
      "Barnet\tLieberman\t1\t20.617629\n",
      "Cedvet\tSunay\t1\t20.617629\n",
      "Rosy\tFingered\t1\t20.617629\n",
      "Mal\tWhitfield\t1\t20.617629\n",
      "Abner\tHaynes\t1\t20.617629\n",
      "Patti\tWaggin\t1\t20.617629\n",
      "Rip\tRandall\t1\t20.617629\n",
      "Camilo\tCarreon\t1\t20.617629\n",
      "Harmon\tKillebrew\t1\t20.617629\n",
      "Rocky\tColavito\t1\t20.617629\n",
      "routed\tloser\t1\t20.617629\n",
      "Shipman\tPayson\t1\t20.617629\n",
      "Deane\tBeman\t1\t20.617629\n"
     ]
    }
   ],
   "source": [
    "num_unigrams = sum(word_counts.values())\n",
    "\n",
    "pair_mutual_information_scores = Counter()\n",
    "for (w1, w2), w1_w2_count in word_pair_counts.most_common():\n",
    "    if w1_w2_count > 0:\n",
    "        w1_prob = word_counts[w1] / num_unigrams\n",
    "        w2_prob = word_counts[w2] / num_unigrams\n",
    "        w1_w2_prob = w1_w2_count / num_bigrams\n",
    "        pair_mutual_information_scores[(w1, w2)] = mutual_information(w1_w2_prob, w1_prob, w2_prob)\n",
    "\n",
    "for (w1, w2), mi in pair_mutual_information_scores.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%f\" % (w1, w2, word_pair_counts[(w1, w2)], mi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre\tDame\t6\t18.032666\n",
      "deja\tvue\t7\t17.810274\n",
      "Pulley\tBey\t6\t17.617629\n",
      "Walnut\tTrees\t6\t17.617629\n",
      "Gratt\tShafer\t6\t17.617629\n",
      "Yugoslav\tClaims\t7\t17.447704\n",
      "Hong\tKong\t11\t17.158197\n",
      "bacterial\tdiarrhea\t7\t16.917189\n",
      "Herr\tSchaffner\t6\t16.710738\n",
      "Farm\tCredit\t7\t16.670096\n",
      "Viet\tNam\t13\t16.617629\n",
      "Monroe\tDoctrine\t6\t16.617629\n",
      "aerated\tlagoon\t7\t16.554619\n",
      "Simms\tPurdew\t12\t16.530166\n",
      "Pathet\tLao\t10\t16.530166\n",
      "crown\tgall\t7\t16.530166\n",
      "Adlai\tStevenson\t6\t16.488346\n",
      "Skeletal\tAge\t6\t16.447704\n",
      "Morton\tFoods\t7\t16.348168\n",
      "El\tPaso\t10\t16.295701\n"
     ]
    }
   ],
   "source": [
    "num_unigrams = sum(word_counts.values())\n",
    "\n",
    "pair_mutual_information_scores = Counter()\n",
    "for (w1, w2), w1_w2_count in word_pair_counts.most_common():\n",
    "    if w1_w2_count > 5:\n",
    "        w1_prob = word_counts[w1] / num_unigrams\n",
    "        w2_prob = word_counts[w2] / num_unigrams\n",
    "        w1_w2_prob = w1_w2_count / num_bigrams\n",
    "        pair_mutual_information_scores[(w1, w2)] = mutual_information(w1_w2_prob, w1_prob, w2_prob)\n",
    "\n",
    "for (w1, w2), mi in pair_mutual_information_scores.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%f\" % (w1, w2, word_pair_counts[(w1, w2)], mi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
